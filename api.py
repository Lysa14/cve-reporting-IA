import requests
import re
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from flask import Flask, request, jsonify

# Télécharger les ressources NLTK nécessaires
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# Charger le modèle spaCy pour la reconnaissance des entités nommées
nlp = spacy.load("en_core_web_sm")

# Fonction pour récupérer les données CVE à partir de l'API NVD
def fetch_cve_data(keyword):
    url = f"https://services.nvd.nist.gov/rest/json/cves/1.0?keyword={keyword}"
    response = requests.get(url)
    if response.status_code != 200:
        return f"Erreur: Statut de la requête {response.status_code}"

    try:
        return response.json()
    except requests.exceptions.JSONDecodeError:
        return "Erreur: La réponse n'est pas un JSON valide"

# Fonction pour nettoyer le texte
def clean_text(text):
    text = BeautifulSoup(text, "html.parser").get_text()  # Suppression des balises HTML
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Suppression des symboles inutiles
    text = text.lower()  # Conversion en minuscules
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = nltk.word_tokenize(text)  # Tokenisation
    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
    return ' '.join(cleaned_tokens)

# Fonction pour extraire les entités nommées du texte
def extract_entities(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

# Fonction pour traiter les données CVE
def process_cve_data(data):
    cve_items = data.get('result', {}).get('CVE_Items', [])
    processed_data = []
    for item in cve_items:
        cve_id = item.get('cve', {}).get('CVE_data_meta', {}).get('ID')
        description = item.get('cve', {}).get('description', {}).get('description_data', [])[0].get('value')
        cleaned_description = clean_text(description)
        entities = extract_entities(cleaned_description)
        processed_data.append({'CVE_ID': cve_id, 'Description': cleaned_description, 'Entities': entities})
    return processed_data

# Fonction pour vectoriser le texte
def vectorize_texts(texts):
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(texts)
    return vectors, vectorizer

# Fonction pour regrouper les textes par similarité
def cluster_texts(vectors, n_clusters=5):
    kmeans = KMeans(n_clusters=n_clusters)
    kmeans.fit(vectors)
    return kmeans.labels_, kmeans.cluster_centers_

# Fonction pour générer des résumés automatiques
def generate_summary(texts, labels, n_clusters=5):
    summaries = []
    for i in range(n_clusters):
        cluster_texts = [texts[j] for j in range(len(labels)) if labels[j] == i]
        summary = ' '.join(cluster_texts[:3])  # Résumer en prenant les 3 premiers textes du cluster
        summaries.append(summary)
    return summaries

# Initialisation de l'application Flask
app = Flask(__name__)

@app.route('/get_cve', methods=['GET'])
def get_cve():
    software = request.args.get('software')
    if not software:
        return jsonify({"error": "Veuillez fournir le nom d'un logiciel."}), 400

    raw_data = fetch_cve_data(software)
    if isinstance(raw_data, str):
        return jsonify({"error": raw_data}), 500

    processed_data = process_cve_data(raw_data)
    texts = [entry['Description'] for entry in processed_data]
    vectors, vectorizer = vectorize_texts(texts)
    labels, cluster_centers = cluster_texts(vectors)
    summaries = generate_summary(texts, labels)
    return jsonify({"summaries": summaries})

if __name__ == '__main__':
    app.run(debug=True, port=5002)